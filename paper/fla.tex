\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xfrac}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgf}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{document}

\title{Radar Scheduling Method based on Iterative Assembly of Movable Multitasks
	\thanks{This work was supported by the Defence Research and Development Canada Ottawa Research Centre. Ottawa, Ontario, K1A 0Z4, Canada}
}

\author{\IEEEauthorblockN{Amos Hebb}
	\IEEEauthorblockA{
		a.hebb@mail.utoronto.ca}
}

\maketitle

\begin{abstract}
	A radar task scheduling method, multitask assembly, is proposed in this paper.
	In this method, tasks are ordered according to some priority function.
	Tasks from this ordering are iteratively added to a schedule at the lowest cost location.
	When adding a task to this location results in a conflict, a multitask is assembled until it can be inserted without conflict.
	Multitask assembly is compared to implementations of four existing algorithms in eight loading scenarios.
	Multitask assembly produces schedules among the lowest cost of simulated methods, at a runtime among the fastest.
	A Python framework is made available for future work.
\end{abstract}

\begin{IEEEkeywords}
	multi-function radar, radar resource management, task scheduling
\end{IEEEkeywords}

\section{Introduction}

Phased array radars are responsible for multiple functions, and each function produces multiple tasks to be processed.
A radar task is described by its zero cost start time $t_{start}$, dwell time $t_{dwell}$, bounds $t_{earliest}$ and $t_{latest}$, and task priority $p$.
In a multi-function radar system, radar resource management includes task prioritization, task selection, and task scheduling.
One objective is to arrange radar tasks into a schedulable sequence without overlaps to be executed.
The computational runtime of this scheduling algorithm is particularly important, as in application hundreds of radar tasks need to be scheduled in tenths of a second.

A scheduling method takes a set of tasks and either assigns these tasks times or drops them to create a conflict-free schedule.
Performance can be evaluated by the total cost $J$, drop-rate $\gamma$, or computational time $T$ to determine the solution.
This is an NP-hard problem.
For this reason, this paper focuses on heuristic scheduling methods which do not aim to find a global optimum, but instead, a good schedule in a reasonable runtime.

\section{Problem Formulation}

The problem is a variant of the single-machine weighted earliness-tardiness scheduling problem with dropping tasks and inserting idle space.
A scheduling method is applied to a set of tasks producing a set of tasks with a $t_{scheduled}$.
$t_{scheduled}$ contains either a time such that all tasks can be executed conflict-free when started at $t_{scheduled}$, or a marker indicating that the task has been dropped.
The objective is to select $t_{scheduled}$ to minimize the total weighted earliness-tardiness and weighted drop penalty of all tasks.

\begin{equation}
	J = \begin{cases}
		(p \times 10)^2                                       & \mathrm{if}~t_{scheduled} = \mathrm{dropped,} \\
		p \times (\frac{(t_{start} - t_{scheduled})}{\tau})^2 & \mathrm{otherwise.}
	\end{cases}
\end{equation}

\section{Existing Algorithms}

%\subsection{All Gas No Brakes}
Methods like \emph{Earliest start time}, and \emph{Earliest deadline} are computationally fast, around $\mathcal{O}(n)$, but produce schedules with a high cost and drop-rate (low $T$, high $J$, $\gamma$).

%\subsection{Shake and Bake}
Methods like \emph{Random shifted start time} and \emph{Dual Side Scheduling} combine a computationally fast
$\mathcal{O}(n)$ function like earliest-start-time with a source of variance, produce and evaluate some number $K$ variants, and select the lowest cost schedule among these variants.

This results in a computational time of $\mathcal{O}(K \times n)$ for $K$ variants. (medium $T$, medium $J$, $\gamma$)
\section{Simulations}

The following algorithms have been scripted and run.
These simulations, and data used to produce these figures, are available at this \href{https://github.com/navh/radar-scheduling/tree/main/data}{github repository}.
\subsection{Earliest Start Time}

Tasks are ordered by start time and scheduled start-to-end.
Once the window is full, the remaining tasks are dropped.

\subsection{Priority Scheduling}

Tasks are ordered by priority and scheduled start-to-end.
Once the window is full, the remaining tasks are dropped.

\subsection{Dwell Time Priority Scheduling}

Tasks are ordered by dwell time divided by priority squared and scheduled start-to-end.
Once the window is full, the remaining tasks are dropped.

\subsection{Random Shifted Start Time}

Tasks are randomly ordered and scheduled start-to-end.
This is repeated for a fixed number of shuffles.
The cost of each shuffle is evaluated.
The lowest-cost shuffle is selected.

\subsection{Dual Side Scheduling}

A fixed number of evenly spaced times within the window are selected as borders.
For each border, tasks with a mean time less than the border are ordered by task end time and scheduled border-to-left.
Tasks with a mean time greater than the border are ordered by start time and scheduled border-to-right.
Once the window is full, the remaining tasks are dropped.

\subsection{Dual Side Scheduling with Random Shifted Start Times}

Tasks are shuffled a fixed number of times, for each shuffle, \emph{Dual Side Scheduling} is applied.

\section{Multitask Assembly}

The main idea behind multitask assembly is that schedules without conflicts are trivial.
Solving a conflict between only two tasks is usually quite obvious.
Two conflicting tasks will have one task either coming before or after the other task with no gap between them.
By only ever inserting one task at a time and treating sequential tasks with no gaps between them as individual tasks a schedule can be produced.

\subsection{Multitask}

A multitask is conceptually identical to a single task.
It has a $t_{start}$ describing the lowest cost starting time and $t_{dwell}$ describing the duration.
A multitask contains an ordered list of tasks, the first task will be scheduled at $t_{scheduled}$, with each subsequent task starting as soon as the preceding task completes.
The $t_{scheduled}$ of a multitask is the $t_{scheduled}$ of its first task, and $t_{dwell} = \sum_{task}^{tasks}t_{dwell}$ is the dwell time of all tasks.

\subsection{Task Selection}

Tasks are first ordered by $\frac{t_{dwell}}{p^2}$, which is the Dwell-Priority Ratio from \cite{poster}, and the highest priority task is always added next.
Once a task has been added to a schedule, it may be moved in time, but it will not be dropped.

\subsection{Decision Loop}

While diverging slightly from implementation, at a high level the algorithm works as follows.

\begin{algorithmic}[1]
	\STATE {tasks $\gets$ all unscheduled tasks}
	\STATE {schedule $\gets \{\} $}
	\STATE {dropped $\gets \{\}$}
	\WHILE{tasks $\neq \{\}$}
	\STATE {dropped $\gets $ tasks with $t_{dwell}$ $>$ remaining window}
	\STATE {$t$ $\gets$ best task from tasks}
	\IF {$t_{dwell}$ conflict free with schedule}
	\STATE {schedule $\gets t$}
	\ELSE
	\STATE{conflict $\gets$ a conflicting task from schedule}
	\ENDIF
	\ENDWHILE
\end{algorithmic}

This is visualized in figure \ref{fig:animation} for a better intuition for how this algorithm schedules tasks.
In figure \ref{fig:animation} initial state has all tasks are candidates.
The top candidate is attempted to be added to the schedule every iteration.
Iteration 2 shows the top candidate being added to the schedule.
Iteration 3 shows a second candidate with no conflict being placed with idle time between them.
Iterations 4-5 show a simple conflict between 2 tasks resulting in a multitask being created and placed as top candidate, then added schedule.
Iterations 6-7 show how a double conflict is solved by only ever adding one task at a time to multitask.
Iteration 8 is the final conflict-free schedule, in this case, a single large multitask.

\begin{figure}
	\centering
	{\input{figs/animation.tex}}
	\caption{Iterations moving top task from queue to schedule}
	\label{fig:animation}
\end{figure}
\section{Results}

Unless otherwise specified, the following figures were produced with a run of 1 million 10-task scenarios.
\emph{Random Shifted Start Time} is configured to run for 1000 iterations.
\emph{Dual Side Scheduling} runs 100 RSS iterations with 10 borders.
These are based on the results presented in \cite{dss_2020}, using 100 RSS iterations instead of 50 to keep $K$ at 1000 for both.

\subsection{Overloaded Cost}

\begin{figure}[h]
	\centering
	{\input{figs/overload5.pgf}}
	\caption{Mean Cost in Overloaded Scenarios}
	\label{fig:overload5}
\end{figure}

\begin{figure}[h]
	\centering
	{\input{figs/overload3.pgf}}
	\caption{Best 3 Methods Mean Cost in Overloaded Scenarios}
	\label{fig:overload3}
\end{figure}

In figure \ref{fig:overload5}, \emph{Earliest Start Time} and \emph{Earliest Deadline} have a much higher normalized cost than the other three algorithms.

Figure \ref{fig:overload3} only has the best 3 methods, when iterative algorithms run for 1000 iterations performance is remarkably similar.
While there are regions where one appears better than another, these are not consistent between runs.
The main takeaway from figure \ref{fig:overload3} is that for overloading scenarios \emph{Multitask Assembly} produces schedules with costs similar to \emph{Random Shifted Start Time} and \emph{Dual Side Scheduling}.

\subsection{Underloaded Cost}

\begin{figure}[h]
	\centering
	{\input{figs/underload5.pgf}}
	\caption{Mean Cost in Overloaded Scenarios}
	\label{fig:underload5}
\end{figure}

Underloaded scenarios present an even more interesting set of tradeoffs.
These tradeoffs are visualized in figure \ref{fig:underload5}.
\emph{Earliest Start Time}, \emph{Earliest Deadline}, and \emph{Multitask Assembly} have a deterministic order in which tasks will be scheduled.
When the loading rate is exactly 1 \emph{Multitask Assembly Scheduling} will produce a single full-window-sized multitask sorted by earliest start time, resulting in a schedule identical to \emph{Earliest Start Time}.
This multitask may not start at 0 if this reduces cost. This is why for near 1 loading rate, \emph{Multitask Assembly} and \emph{Earliest Start Time} converge.

\emph{Random Shifted Start Time} is able to shuffle the sequence of tasks, it appears that simply sampling 1000 random schedules and starting at 0 produces the lowest cost schedules with loading rates from 90\% to 100\%.
\emph{Dual Side Scheduling with Random Shifted Start Time} evaluates fewer random shuffles, and therefore is not as lucky, but is also capable of starting its sequence at points other than 0.
This is a beneficial combination, making it the best choice from about 70\% to 90\%, nearly as performant as \emph{Random Shifted Start Times} right to 100\%.

The ability to insert multiple idle periods and keep tasks with no conflicts exactly at their 0 cost $t_{start}$ sounds powerful, but is not valuable until below 60\% loading.
In underloaded situations, the ability to evaluate multiple schedule orders is more valuable than inserting idle space most schedules.

\subsection{Droprate}

\begin{figure}[h]
	\centering
	{\input{figs/droprate.pgf}}
	\caption{Droprate in Overloaded Scenarios}
	\label{fig:droprate}
\end{figure}

Only \emph{Dual Side Scheduling} is capable of dropping tasks below a 100\% loading rate when $\tau = 1$.
With priorities being integers 1-9 and drop penalty of 1, it does so sometimes during the period where it produces the lowest cost schedules.

Figure \ref{fig:droprate} shows that all algorithms have similar mean drop rates when the number of tasks is fixed.
\emph{Multitask Assembly} is consistently among the lowest, mostly explained by prioritizing shorter tasks and putting them into the schedule first.

\subsection{Computational Time}

\begin{figure}[h]
	\centering
	{\input{figs/comptime5.pgf}}
	\caption{Compute Time}
	\label{fig:comptime5}
\end{figure}

\begin{figure}[h]
	\centering
	{\input{figs/comptime3.pgf}}
	\caption{Compute Time of 3 Fastest Methods}
	\label{fig:comptime3}
\end{figure}

Figure \ref{fig:comptime5} shows \emph{Dual Side Scheduling} and \emph{Random Shifted Start Time} take a consistent 80ms to complete 1000 iterations, about 1000 times longer than the 0.08ms it takes to run \emph{Earliest Deadline} and \emph{Earliest Start Time}.
Focusing on the three faster methods in figure \ref{fig:comptime3} shows that \emph{Multitask Assembly} takes twice as long as these algorithms with only 10 tasks.
The current implementation has a worst-case runtime of $\mathcal{O}(n^2)$ but in implementation has many ways to avoid this such as dropping tasks once they are no longer schedulable.
To test the impact of this potentially quadratic runtime against these linear algorithms, 1000 samples were run on schedules with 1 through 100 tasks.
To reduce their runtime for this sweep, \emph{Random Shifted Start Time} is configured to only check 100 iterations, and \emph{Dual Side Scheduling} only does 10 shuffles with 10 borders each.
Figure \ref{fig:comptimesweep} shows the experimental runtime during these simulations.

\begin{figure}[h]
	\centering
	{\input{figs/comptimesweep.pgf}}
	\caption{Compute time as number of tasks increases}
	\label{fig:comptimesweep}
\end{figure}

The worst case is not realized, as at 100 tasks an $n^2$ should be equal to $K \times n$ when $K = 100$.
Figure \ref{fig:comptimesweep} shows slow quadratic growth at a rate that leaves it below 5 milliseconds for 100 tasks.

\subsection{8 Scenarios}

The states from \cite{modified_q_learn} have been rearranged to the following order.
Mean results for each method in each scenario are compared to the performance of \emph{Earliest Start Time}.

\begin{enumerate}
	\item $ \Sigma t_{dwell} \ge L $ \& $ \bar{t}_{start} \le 0.5 $ \& $ \Sigma $ conflict $ \le 10$
	\item $ \Sigma t_{dwell} \ge L $ \& $ \bar{t}_{start} \le 0.5 $ \& $ \Sigma $ conflict $ > 10$
	\item $ \Sigma t_{dwell} \ge L $ \& $ \bar{t}_{start} > 0.5 $ \& $ \Sigma $ conflict $ \le 10$
	\item $ \Sigma t_{dwell} \ge L $ \& $ \bar{t}_{start} > 0.5 $ \& $ \Sigma $ conflict $ > 10$
	\item $ \Sigma t_{dwell} < L $ \& $ \bar{t}_{start} \ge 0.5 $ \& $ \Sigma $ conflict $ \le 10$
	\item $ \Sigma t_{dwell} < L $ \& $ \bar{t}_{start} \ge 0.5 $ \& $ \Sigma $ conflict $ > 10$
	\item $ \Sigma t_{dwell} < L $ \& $ \bar{t}_{start} > 0.5 $ \& $ \Sigma $ conflict $ \le 10$
	\item $ \Sigma t_{dwell} < L $ \& $ \bar{t}_{start} > 0.5 $ \& $ \Sigma $ conflict $ > 10$
\end{enumerate}

\begin{table}[h]
	\centering
	\begin{tabular}{lrrrrr}
		\hline
		\textbf{State} & \textbf{EST} & \textbf{ED} & \textbf{RSST} & \textbf{DSS} & \textbf{FLA} \\\hline
		1              & 1.0          & 1.076093    & 0.365978      & 0.359640     & 0.373556     \\
		2              & 1.0          & 1.031949    & 0.342289      & 0.343085     & 0.369665     \\
		3              & 1.0          & 1.096770    & 0.352149      & 0.352936     & 0.352667     \\
		4              & 1.0          & 1.083805    & 0.338597      & 0.346299     & 0.384441     \\
		5              & 1.0          & 1.190858    & 0.663880      & 0.560949     & 0.720155     \\
		6              & 1.0          & 1.153737    & 0.586624      & 0.563300     & 0.819483     \\
		7              & 1.0          & 1.157137    & 0.575815      & 0.443852     & 0.517884     \\
		8              & 1.0          & 1.188896    & 0.530837      & 0.500677     & 0.805071     \\ \hline
		\\
	\end{tabular}
	\caption{Eight Scenarios Mean Cost Ratio with EST}
	\label{tab:eight}
\end{table}

\subsection{Computer}

All the simulations and calculations presented in this paper are conducted by using Python 3.11.2 in a macOS Ventura 13.2.1 environment.
The computer employed includes the 1st generation Apple M1 Pro 10 Core, 16 GB RAM, and a solid-state drive.

\section{Takeaways}

There is no framing of this problem that can be solved with a single pass.
Choosing heuristics is difficult, and intuition was frequently violated.
The actual mechanism by which certain decisions improve performance remains elusive.

\subsection{Ignore Idle Time Insertion}

Convinced that one of the advantages of \emph{Dual Side Scheduling} was that it served as a roundabout way to run RSST with various idle times inserted at the start.
Based on this, key design decisions were made when putting together \emph{Multitask Assembly Scheduling}.
The view was that the ability to insert more idle times with greater intention between scheduled tasks would improve upon existing algorithms.
In particular, that idle time insertions would be most impactful under light underloading.

The reality was the opposite, \emph{Multitask Assembly} performed worse in light underloading scenarios, and performs better in higher loading scenarios where it produces a gap-free schedule indistinguishable from a `front-to-back' approach.
After bolting on many additional heuristics, the result is a needlessly complex dwell time ratio algorithm.
More care taken to not prematurely drop tasks does provide some benefit, but could easily be added to any of the other algorithms.

\subsection{Experimental Design}

Smaller iterations on existing algorithms would allow me to more confidently make claims about what worked and what did not work.
There are at least 3 ideas here.
First, the idea of iteratively inserting new tasks by evaluating tasks already committed.
Second, deciding on an order but not a start time for a group of tasks, and then scheduling these as if they are a single task.
Finally, adding the ability to leave idle time between scheduled tasks.

My intuition based on these results, but mostly my reading about branch-and-bound, is that iterative approaches should be explored further.
Idle time, despite being so obvious to me, was empirically not as helpful as my intuition tells me it should be.
The concept of a movable multitask, the main idea I was hoping to explore with this work, is still something I am uncertain about.

A decisive answer could have been produced by doing smaller modifications to existing algorithms and evaluating these ideas on their own.
Instead, the analysis of these ideas is muddied by the many unjustified heuristics: dwell time priority squared as sorting method, changing priorities from 0.1-0.9 to 1-9, or adding tasks even when it increases cost, sorting sequences by start time, and using a weighted mean as a multitasks desired start time.
The result is a reasonably performant algorithm which provides fairly little insight into the impact of these decisions on their own.

\subsection{Branch and Bound}

The current best approaches involve breaking the problem into smaller problems but have abandoned all hope of selecting a single global heuristic that always selects the next best addition to a schedule.
They instead consider all candidates and estimate the upper and lower bounds of these sub-problems, only discarding hopeless sub-problems.
This is the most commonly employed approach for solving NP-hard optimization problems.
Performant solutions are found with both integer linear programming and constraint programming.
These approaches are still large in both computational and memory complexity, limited mostly by the heuristic used to estimate bounds.
Industrial engineering research on the one-machine earliness tardiness scheduling problem is most relevant.

\section{Future Work}

This particular algorithm does not warrant further investigation.
While it is performant and there are many opportunities to further refine by evaluating changes to the many heuristics,
the underlying `single-pass' design decision limits the upper bound of performance improvements across many scenarios.

\subsection{Reinforcement Learning}

There are far simpler scheduling problems which remain unsolved, and the extreme time constraints in the radar environment mean that some heuristic-based approach must be employed.
It is simple to generate billions of scenarios, at least when simply selecting randomly from totally flat distributions.
Using a reinforcement learning algorithm to develop a more exotic heuristic paired with a tree search is an attractive direction for future work.

\subsection{Python Sandbox}

\href{https://github.com/navh/radar-scheduling/tree/main/}{https://github.com/navh/radar-scheduling/tree/main/}

The most valuable artifact of this work is the Python framework developed which allows for quick iterations of scheduling algorithm ideas in the context of radar tasks.
Defining tasks and schedules allows for scheduling code to be written much closer to the plain-text description.
It should be helpful not just for evaluating existing known algorithms and developing new ones, but also for integrating with reinforcement learning libraries in the future.

%\section*{Acknowledgment}

\bibliographystyle{IEEEtran}
\bibliography{fla}

\end{document}
